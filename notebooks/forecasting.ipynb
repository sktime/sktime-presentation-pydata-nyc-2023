{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97609c33",
   "metadata": {},
   "source": [
    "# Forecasting With Classical and Machine Learning Methods Using sktime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d4bd32",
   "metadata": {},
   "source": [
    "### What is Forecasting?\n",
    "\n",
    "Forecasting is the process of making predictions about future events or trends by analyzing past and present data.\n",
    "\n",
    "Forecasts are built by studying time series data.\n",
    "\n",
    "What is time series data?\n",
    "\n",
    "time series = recorded observations of one object or process at different time points.\n",
    "\n",
    "observations at different time points are of same kind/type.\n",
    "\n",
    "observations recorded with time index (= recorded time stamp)\n",
    "\n",
    "![](../images/problem_statement.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080ecad2",
   "metadata": {},
   "source": [
    "![](../images/presentation_agenda.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ca175d",
   "metadata": {},
   "source": [
    "![](../images/time_series_definition.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55063bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sktime.datasets import load_shampoo_sales\n",
    "\n",
    "y = load_shampoo_sales()\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c067b3",
   "metadata": {},
   "source": [
    "What makes time series problems unique?  \n",
    "\n",
    "sequentiality, auto-correlation!\n",
    "\n",
    "observations depend on previous value, e.g., passengers this year similar to last year, but not similar to 50 years ago\n",
    "\n",
    "vs tabular ML (sklearn): data can be shuffled without altering statistical properties\n",
    "(\"independent and identically distributed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a15a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# airlines data\n",
    "from sktime.utils.plotting import plot_series\n",
    "\n",
    "fig, ax = plot_series(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49250e32",
   "metadata": {},
   "source": [
    "autocorrelation estimator shows adjacent values are very correlated!\n",
    "\n",
    "e.g., observations 1 period away almost perfectly correlate!\n",
    "\n",
    "(side note for stats aficionados: usually you apply acf after differencing or stationary series, this is for illustrating autocorrelation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f511906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "sm.graphics.tsa.plot_acf(y, lags=24, ax = ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08a740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for comparison, here's what this plot would look like if you shuffle this data\n",
    "import numpy as np\n",
    "\n",
    "shuffled_data = np.random.permutation(y)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "sm.graphics.tsa.plot_acf(shuffled_data, lags=24, ax = ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834d2be3",
   "metadata": {},
   "source": [
    "ML models (sktime) typically built on the assumption that data is i.i.d.  \n",
    "\n",
    "Time series models need to model temporally dependent data, trend, auto-correlation.  \n",
    "\n",
    "Common \"classical\" examples of time series models: Exponential Smoothing, ARIMA, Theta, etc.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eb0919",
   "metadata": {},
   "source": [
    "### Classical models, ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b3b6e2",
   "metadata": {},
   "source": [
    "\"classical\" models standard for forecasting up until today!\n",
    "\n",
    "ML models with large parameters frequently underperform\n",
    "\n",
    "#### why? theory:\n",
    "\n",
    "auto-correlation reduces \"effective sample size\", roughly by 1 divided auto-correlation integral (!)\n",
    "\n",
    "ML and DL models need large sample size for training\n",
    "\n",
    "ineffective on low effective sample size (pre-training, global models can be way out - more later!)\n",
    "\n",
    "##### case in point:\n",
    "\n",
    "Monash Forecasting Repository. https://forecastingdata.org/\n",
    "\n",
    "30 datasets study. Classical time series regularly outperform the \"latest\" models (dataset dependent).\n",
    "\n",
    "Time series models often outperform more contemporary techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006e7287",
   "metadata": {},
   "source": [
    "![Monash Forecasting Results](../images/monash_respository.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64422c9",
   "metadata": {},
   "source": [
    "However, there are compelling reasons to want to use ML for forecasting problems.  \n",
    "\n",
    " - Ability to recognize non-linear patterns\n",
    " - Ability to incorporate large amounts of non-time based exogenous data\n",
    " - Ability to capture global patterns among many time series\n",
    " - Recent successes using ML in forecasting competitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9045f7f",
   "metadata": {},
   "source": [
    "![](../images/m5_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae34026",
   "metadata": {},
   "source": [
    "![](../images/m5_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7b153e",
   "metadata": {},
   "source": [
    "Using ML for forecasting problems presents some issues.\n",
    "\n",
    "Data has to undergo pre-processing to make it usable for forecasting.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdd435d",
   "metadata": {},
   "source": [
    "### How to use both in practice? ``sktime``'s unified interface!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6886d6e6",
   "metadata": {},
   "source": [
    "time series ecosystem:\n",
    "\n",
    "* fragmented, inhomogehous\n",
    "* **many** choices for time series models - in theory and in python packages\n",
    "* **inhomogenous interfaces**, pain to try/use different kinds of models\n",
    "\n",
    "``sktime`` provides easy, flexible, unified interface for time series modelling!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9042f773",
   "metadata": {},
   "source": [
    "![sktime](../images/unified_framework.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2351de03",
   "metadata": {},
   "source": [
    "forecasting with ``sktime`` is simple!\n",
    "\n",
    "let's pretend we want to predict shampoo sales, typical business use case (and toy data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106dd036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.datasets import load_shampoo_sales\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sktime.utils.plotting import plot_series\n",
    "\n",
    "y = load_shampoo_sales()\n",
    "\n",
    "y_train, y_test = temporal_train_test_split(y=y, test_size=6)\n",
    "fig, ax = plot_series(y_train, y_test, labels=[\"train\", \"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd5eb83",
   "metadata": {},
   "source": [
    "as simple as this:\n",
    "\n",
    "1. specify model\n",
    "2. fit data\n",
    "3. predict\n",
    "\n",
    "(there's of course much more - evaluate, update/stream, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bd6c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.arima import AutoARIMA\n",
    "\n",
    "# 1) Specify the model\n",
    "forecaster = AutoARIMA()\n",
    "\n",
    "# 2) Fit on train data\n",
    "# need to specify \"forecasting horizon\" = where to forecast\n",
    "fh = [1, 2, 3, 4, 5, 6] # Relative to y_train\n",
    "forecaster.fit(y_train, fh)\n",
    "\n",
    "# 3) Use fitted model to predict for a certain forecast horizon\n",
    "y_pred = forecaster.predict()\n",
    "\n",
    "fig, ax = plot_series(y_train, y_test, y_pred, labels=[\"train\", \"test\", \"pred\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9253a1c",
   "metadata": {},
   "source": [
    "this is the same for any model in sktime!\n",
    "\n",
    "classical (ARIMA etc), ML (reduction), deep learning (torch transformers), your own custom models..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ad9f4f",
   "metadata": {},
   "source": [
    "* list predefined ones via `all_estimators` or check the API reference\n",
    "* add your own forecaster in third party repo via extension template\n",
    "* or use \"building blocks\" - first and third party - to build your custom model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a05ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.registry import all_estimators\n",
    "\n",
    "all_estimators(estimator_types=\"forecaster\", as_dataframe=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858bb1a6",
   "metadata": {},
   "source": [
    "### using ML models for forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64504f4",
   "metadata": {},
   "source": [
    "How to prepare time series data for ML?\n",
    "\n",
    "ML models don't have an innate ability to \"see\" previous samples in the data.  \n",
    "\n",
    "![tabularization](../images/tabularization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6fb551",
   "metadata": {},
   "source": [
    "The following code will recreate the above diagram and fit it to a histogram based gradient boosting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75f1af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "\n",
    "naive_forecaster = NaiveForecaster(strategy = 'last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60120949",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sktime.forecasting.compose import make_reduction\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sktime.performance_metrics.forecasting import MeanAbsolutePercentageError\n",
    "\n",
    "smape = MeanAbsolutePercentageError(symmetric=True)\n",
    "\n",
    "y_train, y_test = temporal_train_test_split(y=y, test_size=6)\n",
    "\n",
    "fh = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# Can be swapped with XBGoost, LightGBM, CatBoost, etc.\n",
    "regressor = HistGradientBoostingRegressor()\n",
    "\n",
    "# Create a forecaster from the tabular regressor by wrapping it in `make_reduction`\n",
    "forecaster = make_reduction(regressor, strategy=\"direct\", window_length=16)\n",
    "\n",
    "y_pred = forecaster.fit_predict(y=y_train, fh=fh)\n",
    "title = f\"Gradient-boosted tree regressor - sMAPE error: {smape(y_test, y_pred):.1%}\"\n",
    "fig, ax = plot_series(\n",
    "    y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"], title=title\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e012496",
   "metadata": {},
   "source": [
    "the `regressor` algorithm can be any estimator that has a `fit` and `predict` method.  \n",
    "\n",
    "The `make_reduction` function transforms previous values into exogenuous variables.\n",
    "\n",
    "However, a quick eyeball test reveals these predictions to be unsatisfactory.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b1d81f",
   "metadata": {},
   "source": [
    "It also does not outperform an AutoARIMA model out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0da963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.arima import AutoARIMA\n",
    "\n",
    "arima_forecaster = AutoARIMA(sp=12, d=0, max_p=2, max_q=2, suppress_warnings=True)\n",
    "y_pred_arima = arima_forecaster.fit_predict(y=y_train, fh=fh)\n",
    "\n",
    "print(f\"Gradient-boosted tree regressor - sMAPE error: {smape(y_test, y_pred):.1%}\")\n",
    "print(f\"AutoARIMA - sMAPE error: {smape(y_test, y_pred_arima):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd54ca8",
   "metadata": {},
   "source": [
    "Why is the GBM under-performing?:\n",
    "\n",
    "- Gradient boosting trees cannot \"extrapolate\"\n",
    "- only forecast well within their observed range\n",
    "\n",
    "Solution: make (more) stationary by differencing\n",
    "\n",
    "easy to do in `sktime`: transformers (= transformation estimators)\n",
    "\n",
    "(note: wider concept than deep learn transformers, includes simple trafos too)\n",
    "\n",
    "Let's see how to use the `Differencer` transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf9d58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.series.difference import Differencer\n",
    "\n",
    "transformer = Differencer(lags=1)\n",
    "y_transform = transformer.fit_transform(y)\n",
    "fig, ax = plot_series(\n",
    "    y, y_transform, labels=[\"y\", \"y_diff\"], title=\"Difference transformation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c954480b",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = HistGradientBoostingRegressor()\n",
    "forecaster = make_reduction(regressor, strategy=\"direct\", window_length=16)\n",
    "forecaster_with_differencer = Differencer(lags=1) * forecaster\n",
    "\n",
    "# to do:  use transformed target forecaster for this step\n",
    "y_pred = forecaster_with_differencer.fit_predict(y=y_train, fh=fh)\n",
    "title = f\"Gradient-boosted tree regressor with difference transform - sMAPE error: {smape(y_test, y_pred):.1%}\"\n",
    "fig, ax = plot_series(\n",
    "    y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"], title=title\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d585a642",
   "metadata": {},
   "source": [
    "We would like to take this example extend it to account for more complicated scenarios:  \n",
    "\n",
    " - Forecasting many time series simultaneously, many of which may be interrelated.\n",
    " - Producing features beyond using lag values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d28898c",
   "metadata": {},
   "source": [
    "### Panel Forecasting With ML and sktime\n",
    "\n",
    "The examples so far have focused on a single time series.\n",
    "\n",
    "However, most real world problems often involve multiple time series, many of which exist in a hierarchy.\n",
    "\n",
    "![hierarchical time series](../images/hierarchy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c717f57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example hierarchical time series\n",
    "from pydata_utils import load_product_hierarchy\n",
    "\n",
    "y = load_product_hierarchy()\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492547cd",
   "metadata": {},
   "source": [
    "A key component of hierarchical time series is that many of the individual time series are related to one another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442b5dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_index = y.droplevel(-1).index.unique()\n",
    "fig, ax = plot_series(*(y.loc[idx] for idx in product_index), labels=product_index, title=\"Product sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fd2baa",
   "metadata": {},
   "source": [
    "Traditionally, each individual time series would be modeled separately.  \n",
    "\n",
    "This approach potentially misses the shared properties of the different time series, making their modeling inefficient.  \n",
    "\n",
    "For ML models, a better approach is often to fit all of the time series jointly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc51dc8c",
   "metadata": {},
   "source": [
    "Simultaneously fitting many time series is known as global forecasting.\n",
    "\n",
    "It's been the approach that's been successfully used with ML models in competitions.\n",
    "\n",
    "Using global approaches to model many time series is straight forward in sktime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71a0ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the train and test sets\n",
    "y_train, y_test = temporal_train_test_split(y, test_size=4)\n",
    "\n",
    "# to do:  add in differencing w/ transformed targetforecaster\n",
    "regressor         = HistGradientBoostingRegressor()\n",
    "forecaster_local  = make_reduction(regressor, strategy=\"direct\", window_length=12, pooling=\"local\")\n",
    "forecaster_global = make_reduction(regressor, strategy=\"direct\", window_length=12, pooling=\"global\")\n",
    "forecaster_arima  = AutoARIMA(sp=12, d=0, max_p=2, max_q=2, suppress_warnings=True)\n",
    "\n",
    "hier_smape = MeanAbsolutePercentageError(symmetric=True, multilevel=\"raw_values\")\n",
    "\n",
    "y_pred_local = forecaster_local.fit_predict(y_train, fh=[1, 2, 3, 4])\n",
    "y_pred_global = forecaster_global.fit_predict(y_train, fh=[1, 2, 3, 4])\n",
    "y_pred_arima = forecaster_arima.fit_predict(y_train, fh = [1, 2, 3, 4])\n",
    "\n",
    "errors_local = hier_smape(y_test, y_pred_local)\n",
    "errors_global = hier_smape(y_test, y_pred_global)\n",
    "errors_arima = hier_smape(y_test, y_pred_arima)\n",
    "\n",
    "print(f\"Average sMAPE with local pooling: {errors_local.mean().iloc[0]:.1%}\")\n",
    "print(f\"Average sMAPE with global pooling: {errors_global.mean().iloc[0]:.1%}\")\n",
    "print(f\"Average sMAPE with AutoARIMA: {errors_arima.mean().iloc[0]:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd08ac9d",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "Machine learning models often benefit from feature engineering beyond lags\n",
    "\n",
    "Examples:\n",
    " - Summary statistics for a given time series\n",
    " - Window statistics for a given time series\n",
    "\n",
    "Producing these features can easily be added to sktime pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1156ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.series.summarize import WindowSummarizer\n",
    "\n",
    "# arguments for the window transformer\n",
    "kwargs = {\n",
    "    \"lag_feature\": {\n",
    "        \"lag\": [1, 2, 3],\n",
    "        \"mean\": [[1, 3]],\n",
    "        \"std\": [[1, 10]],\n",
    "        \"kurt\": [[1, 10]]},\n",
    "    \"truncate\": 'bfill'\n",
    "}\n",
    "\n",
    "summarizer = WindowSummarizer(**kwargs, n_jobs = 1)\n",
    "window_data = summarizer.fit_transform(y_train)\n",
    "window_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec535719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# window statistics can capture local patterns in a time series\n",
    "# not easily observed by lag values alone\n",
    "fig, ax = plot_series(*(window_data.loc[idx, 'Sales_kurt_1_10'] for idx in product_index), \n",
    "                      labels=product_index, title=\"Skewness of Product Sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53100927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.series.summarize import WindowSummarizer\n",
    "\n",
    "# arguments for the window transformer\n",
    "kwargs = {\n",
    "    \"lag_feature\": {\n",
    "        \"lag\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "        \"mean\": [[1, 3]],\n",
    "        \"std\": [[1, 10]],\n",
    "        \"kurt\": [[1, 10]]},\n",
    "    \"truncate\": 'bfill'\n",
    "}\n",
    "\n",
    "forecaster_window = make_reduction(\n",
    "    regressor,\n",
    "    transformers=[WindowSummarizer(**kwargs, n_jobs=1)],\n",
    "    window_length=None,\n",
    "    strategy=\"direct\",\n",
    "    pooling=\"global\",\n",
    ")\n",
    "\n",
    "y_pred_global_window = forecaster_window.fit_predict(y_train, fh=[1, 2, 3, 4])\n",
    "errors_global_window = hier_smape(y_test, y_pred_global_window)\n",
    "\n",
    "print(f\"Average sMAPE with local pooling: {errors_local.mean().iloc[0]:.1%}\")\n",
    "print(f\"Average sMAPE with global pooling: {errors_global.mean().iloc[0]:.1%}\")\n",
    "print(f\"Average sMAPE with AutoARIMA: {errors_arima.mean().iloc[0]:.1%}\")\n",
    "print(f\"Average sMAPE with window transformations: {errors_global_window.mean().iloc[0]:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdee28d",
   "metadata": {},
   "source": [
    "As we can see in the above example, we've now beat the AutoARIMA baseline.  \n",
    "\n",
    "Global forecasting also has faster fitting times than local forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d293ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit forecaster_window.fit(y_train, fh=[1, 2, 3, 4])\n",
    "%timeit forecaster_arima.fit(y_train, fh=[1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3d1cfe",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "What did we cover today?  \n",
    "\n",
    " - Time series models and machine learning models can both successfuly be used for forecasting\n",
    " - Time series models are easier to use \"out of the box\" on a single time series\n",
    " - ML models can make more sense for modeling time series simultaneously\n",
    " - You should expect to do feature engineering when time series modeling with ML\n",
    " - `sktime` provides a helpful interface for unifying the workflows for the tasks described today"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
